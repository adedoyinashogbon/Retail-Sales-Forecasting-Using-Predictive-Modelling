Rule-Based Specification for Report Structure and Content
The following is a structured set of rules and requirements for an automated system or script to verify the completeness and organization of a final research report on predictive modelling for retail sales forecasting. These rules formalize the expected sections, order, and content constraints of the report, in line with CST3990 module expectations:
Document Length and Formatting:
Word Count: The report must contain approximately 15,000 words (±10% or as specified by the supervisor), excluding the reference list and appendices​

. It should not exceed 18,000 words without supervisor approval.
Page Format: Use A4 page size with standard margins. Text should be 1.5 line-spaced in a clear 12-point font (Arial or Times New Roman or equivalent)​

. Page numbers should be included on all pages (except perhaps the title page).
Style Consistency: All headings, sub-headings, and body text should follow a consistent style (e.g., font type and size for each level of heading, as per any provided template or style guide). The report should be written in formal academic English, with correct grammar and spelling (this contributes to the Style assessment criterion​
).
Referencing Style: A recognized academic referencing style (e.g., Harvard or IEEE) must be used consistently throughout. In-text citations and the bibliography must be properly formatted and cross-checked for consistency (every citation has a reference, every reference is cited at least once). The bibliography should demonstrate correct and almost error-free use of the chosen style​
.
Front Matter (Order as listed):
a. Title Page – Present as the first page, including: project title; author name (student); student ID; degree and department; university name; supervisor’s name; submission date. This page should have no page number (or is numbered i in Roman numeral if front matter is numbered separately).
b. Declaration of Original Work – A statement of originality or honour code must appear early in the report (either on the title page or the next page). It should include the exact declaration required (e.g., “I hereby confirm that the work presented in this report is wholly my own…”​

) and be signed and dated by the student (digitally or handwritten as per guidelines).
c. Acknowledgements – (Optional) If included, this page comes after the declaration. It should be brief (a few sentences) thanking individuals or organizations that assisted in the project.
d. Abstract – A single paragraph summary of the entire project, typically ~200–300 words. It must succinctly state the research problem, the methods used, the key results, and conclusions. The abstract should effectively encapsulate the report without being too short or overly lengthy​

. No citations are usually needed in the abstract.
e. Table of Contents – Auto-generated list of all section headings and subheadings with page numbers. Ensure that the section titles in the TOC exactly match those in the text and that page numbers are correct. This is followed (if applicable) by a List of Figures and a List of Tables, each on a new page, listing figure captions and table titles with their page numbers. These lists help navigate the document and should include every figure/table in the report.
Introduction (begin new page, and count as Section 1):
Purpose: This section must introduce the research topic of retail sales forecasting and set the context. It should clearly articulate the problem statement or research question that the project addresses (e.g., improving accuracy of sales predictions for retail stores using AI models).
Background and Rationale: The introduction should briefly explain why this problem is important (for businesses, for the field of AI, etc.), providing a rationale for choosing this topic​

. It should mention any real-world challenges (e.g., demand variability, the impact of promotions) to justify the need for advanced predictive modelling.
Aim and Objectives: There must be a concise statement of the project’s aim, followed by a bulleted or clearly delineated list of specific objectives. Objectives should be measurable and cover both the development of the predictive model and its evaluation (for example: Objective 1: Build a predictive model (e.g., using machine learning) to forecast weekly sales; Objective 2: Evaluate the model’s performance against existing benchmarks; Objective 3: Analyze results to provide insights into forecasting improvements).
Scope and Limitations (optional): The introduction may define the scope of the work – what is included or excluded. For instance, clarify if the project focuses on one retailer, one type of product, or a certain time horizon, and mention any major limitations upfront if they are evident (e.g., “This project does not address supply chain factors beyond sales data”).
Structure Overview: The end of the Introduction must contain a brief description of how the report is structured. Example: “The report is organized as follows: Chapter 2 reviews related work, Chapter 3 describes the methodology, Chapter 4 presents results and discussion, and Chapter 5 concludes the study.” Each major section should be mentioned. This prepares the reader and adds to the logical presentation of the report.
Literature Review (Background):
Placement and Title: This should be a main section (e.g., Section 2) titled “Literature Review” or “Background Research” (the title can vary, but it must clearly indicate a review of existing knowledge).
Content Requirements: This section must survey relevant academic and industry sources on retail forecasting and predictive modelling. It should cover at minimum:
Existing Methods: Describe traditional forecasting methods (like ARIMA, exponential smoothing) and more recent AI/ML approaches (machine learning regression, neural networks, deep learning models for time series) that have been applied to retail sales data.
Key Findings from Literature: Summarize what previous studies have found – for example, typical forecast accuracy ranges, important factors (holidays, economic indicators), and known challenges (cold-start for new products, sudden demand spikes).
Critical Analysis: The review should not just enumerate sources, but also compare and critically evaluate them. The text should identify contrasts or contradictions between studies, methodological strengths/weaknesses, and gaps in the research. Evidence of critical reflection is required (this aligns with high rubric marks: e.g., an excellent literature review “shows critical reflection” on the state of the art​

). The tone should be analytical, pointing out things like “Most studies use MAPE as an error metric, which can be problematic for very low sales values​

,” or “While Smith (2023) achieved high accuracy with algorithm X on electronics sales, this method might not capture seasonality as well as Y method.”
Relevance to Project: The section should conclude by highlighting how the reviewed literature underpins the project. This means identifying a gap or opportunity that the project will address. For instance, “Few studies have applied deep learning to fashion retail data; this project will explore an LSTM-based approach in that context.” This transition sets up the rationale for the methodology in the next section.
Citations: All statements about prior work must be supported by citations from credible sources (academic papers, industry reports, etc.). A good literature review will have multiple citations, indicating wide reading. The referencing should be correct and consistent. The presence of recent sources (ideally including some from the last 3-5 years) is expected to show up-to-date knowledge.
Length: As a guideline, the literature review may comprise roughly 20-30% of the report length (several thousand words), ensuring depth. It can be broken into sub-sections (e.g., “2.1 Traditional Forecasting Methods”, “2.2 Machine Learning Approaches”, “2.3 Retail Forecasting Challenges”) for clarity, but these sub-sections should flow logically.
Methodology:
Section Title: Use a clear title such as “Methodology”, “Methodology and Implementation”, or “Approach”. This should be a main section (e.g., Section 3).
Overview: Begin with a short overview of the chosen approach to solving the problem. For example, state whether this is an experimental study using historical data and predictive modeling, and outline the steps (data collection → data preprocessing → model development → evaluation).
Data Description:
Clearly describe the dataset(s) used for model training and testing. This includes data source (e.g., “historical sales data from Company X’s stores”), the time period covered, frequency (daily, weekly, monthly sales), and the variables available (features such as date, sales, promotions, etc.).
Mention data size (number of records, number of stores/products included). If multiple datasets were combined (e.g., sales data + economic indicators), describe each.
Data Preparation: Explain any preprocessing steps: handling missing values, removing anomalies, normalization or scaling of features, encoding categorical variables (if any, such as store locations or product categories), and splitting data into training/validation/test sets. This process should be reproducible and justified. For instance, “the last 12 weeks of data were held out for testing to evaluate out-of-sample performance.”
Predictive Modelling Technique:
Specify the algorithms or models implemented. For each major model, provide details such as architecture (for neural nets: layers, activation functions; for tree-based models: number of trees, depth; etc.) and any important hyperparameters.
Justification: For each method used, include a brief justification linked to literature or problem needs. For example, “An LSTM was used due to its strength in capturing time-series patterns; this choice is supported by similar successful applications in sales forecasting by Doe (2024).” If multiple models are compared, note this (e.g., “Both a SARIMA model and a Prophet model were first applied as baselines, then a neural network was developed for potential improvement.”). Justifying methodology choices is essential for demonstrating understanding and will satisfy part of the critical evaluation of methodology.
Experimental Design:
Explain how the experiments or model training were conducted. Include information on training epochs, cross-validation strategy (if used), and any parameter tuning. If you used any specific technique like grid search for hyperparameters or early stopping in training, mention it.
State any evaluation procedure decided at this stage: e.g., “Forecast accuracy will be evaluated on test data using MAE and RMSE, and compared against a naive forecast (last week = next week) as a baseline.” Defining this here ensures the reader knows how you will measure success.
Tools and Environment: Mention the development tools (software libraries like Python’s scikit-learn, TensorFlow, R forecast package, etc.) and environment details (for example, “Jupyter Notebook, Python 3.9, on Windows 10” or “Google Colab with GPU support”). This can usually be brief, but it confirms reproducibility elements.
Ethical Considerations: If not already addressed, include a statement on ethics in the methodology. For instance, if the data is proprietary, note permission to use it. If data involves personal information, confirm compliance with privacy laws or anonymization. If no human/identifiable data is involved, state that the project was assessed as low-risk and an ethics approval was not required, or that an ethics form was submitted and approved (with reference to appendix). This ensures the report meets ethical standards set out in the module.
The methodology section should allow a knowledgeable reader to understand exactly what was done and potentially replicate it. It will be assessed for how well it is described and justified, not just what was done. Clarity and completeness here directly contribute to the overall quality (poor methodology description can lead to lower marks even if the project work was good).
Results:
Section Scope: This section (e.g., Section 4) should detail the outcomes of the methodology – the performance of the predictive models and any observations from the experiments. It should stick to factual findings (with minimal interpretation, which will come in the discussion, though some combination is acceptable).
Data Presentation:
Present quantitative results clearly. Use tables to list error metrics for each model or scenario (for example, a table of MAE, RMSE, MAPE for Model A vs Model B). Each table should have a caption and be referenced in the text.
Use figures for visual results: e.g., a plot showing actual vs predicted sales over time, or a bar chart of model comparison. Figures should be high quality and appropriately labeled (axes titles, legend if multiple lines, etc.).
If the project includes qualitative results (perhaps feedback from domain experts on forecast utility), summarize those as well, though primarily this project will yield quantitative results.
Statistical Analysis: If applicable, include statistical significance testing of results (e.g., using a t-test to see if error reductions are significant, or confidence intervals for forecast errors). This is more advanced but could be relevant if comparing models.
Result Coverage: Ensure all objectives have corresponding results. For instance, if one objective was to compare approaches, results for each approach should be reported. If an objective was to identify factors affecting sales, include results of that analysis (like feature importance from a model).
Clarity: The results should be organized, possibly with sub-sections if multiple sets of results exist (e.g., “4.1 Baseline Model Results”, “4.2 AI Model Results”). However, keep the section structured so that a reader can match each result back to the method that produced it.
Avoid discussion in this sub-section beyond brief comments; save extended interpretation for the Discussion section (if separating them). However, minor commentary like “Model X had lower error than Model Y, indicating better capture of weekly patterns” is fine to note alongside the result.
Discussion:
Purpose: This section interprets and evaluates the results in the context of the research questions and existing literature. (If the report is structured as “Results and Discussion” combined, then the content of this rule applies within that combined section. But ensure that both presentation of results and their discussion are clearly addressed.)
Key Points to Address:
Interpretation of Results: Explain what the results mean. For example, discuss why one model outperformed another (was it due to its ability to capture seasonality or non-linear trends?), or why overall errors might be higher or lower than expected.
Comparison to Literature: Critically compare your findings to those in the literature review. Are your results consistent with previous studies (e.g., “Consistent with prior research, the machine learning model outperformed the statistical baseline on volatile sales data”)? Or do they differ (“In contrast to Smith (2022), our results suggest that adding weather data did not significantly improve forecast accuracy, which could be due to the regional focus of our data”)? A strong discussion will reference back to the state of the art to contextualize the significance of the results​

.
Implications: Discuss the practical or theoretical implications. For instance, what would these results mean for a retail manager? (“The improved accuracy could reduce overstock by 10% based on error reduction.”) Or for the field? (“This demonstrates the viability of deep learning even for relatively short historical datasets, which was previously uncertain.”)
Limitations: Identify and acknowledge limitations in your work that could have affected the results. Examples: limited data quantity, certain assumptions in modeling, computational constraints, etc. Explain how each limitation might have influenced outcomes (e.g., “Because the training period was only one year, the model may not have learned long-term seasonal patterns, limiting forecast accuracy for holiday periods.”).
Validity and Reliability: Comment on how reliable your results are. If you ran only one train-test split, results might vary with different splits—did you use enough validation? If the data had random variations, did you average results over multiple runs? Ensuring the reader knows how much confidence to place in the findings is important for scientific rigor.
Critical Evaluation: This section should showcase critical thinking. It’s not just stating results but examining why they occurred and what can be learned. This is directly tied to the assessment criteria for evaluation: an excellent discussion provides convincing justification of the obtained results and draws insightful conclusions​

. Conversely, a shallow discussion that just restates results will score lower.
Reflection: Include a brief reflection if appropriate – what you, as the researcher, learned from doing this project. For example, “The project underscored the importance of data cleaning, as initial results were poor until outliers were removed.” While not always a formal requirement, reflective insight can demonstrate a deeper engagement with the work.
Structure: The discussion can be one unified section or broken into parts (like “5.1 Interpretations, 5.2 Limitations, 5.3 Future Work”) for readability. Ensure it reads as a coherent argument leading to the conclusion that follows. Avoid introducing new results; focus on analyzing those already presented.
Conclusion:
Summary of Findings: The conclusion must concisely recap the research problem and how it was addressed, summarizing the main findings or outcomes. It should tie directly back to the aims stated in the introduction, confirming whether they were achieved.
Contributions: Clearly state the project’s contributions to knowledge or practice. This could be reiterating improvements achieved (e.g., “developed a novel forecasting model that improved accuracy by X% over known methods”) or insights gained (“identified key factors influencing retail sales predictions”). The conclusion should highlight these significant contributions, as the rubric expects an explanation of the project’s value in terms of theory or application​

.
Recommendations/Future Work: Provide specific suggestions for future work, building on the project’s experience and limitations. This might include testing the model on different data (other industries or larger timespans), trying alternative algorithms (e.g., transformers for time-series), or implementing the solution in a real-world pilot to gauge impact. Recommendations show that you recognize the project’s place in a continuous research trajectory.
Closing Statement: End with a strong concluding remark about the project’s overall success or the lesson learned. This should give a sense of closure. For example, a concluding sentence might be, “In summary, this project demonstrates the effectiveness of machine learning for retail forecasting, paving the way for more data-driven inventory decisions.”
Length: The conclusion is usually succinct (around 5-8% of the report). It should not introduce any new data or citations that weren’t previously discussed; it relies on evidence presented earlier.
References:
Content: Every source cited in the report must be listed in the References section (or Bibliography). The list should include all academic papers, books, websites, datasets, and other resources referenced. Do not include sources not cited in the text (no “bibliography for further reading”; only include what was actually referenced).
Format: The reference list must follow the chosen referencing style exactly. For example, in Harvard style: authors, year, title, journal, volume(issue), pages for articles; or for websites: author/organization, year, title, URL, access date. In IEEE: numbered references in order of citation. Whichever style is used, the formatting of each entry (punctuation, italicization, etc.) should be consistent and correct.
Order: References should be ordered appropriately (alphabetically by author for Harvard/APA, or numerically by citation order for IEEE).
Quality: The sources should predominantly be credible and relevant (peer-reviewed papers, conference proceedings, reputable industry reports). Having an adequate number of recent sources (including 2020s publications) is expected.
Cross-check: An automated check should confirm that every in-text citation corresponds to an entry in this list and that there are no orphan entries. Also, the system can check for common formatting errors (e.g., missing italics for journal names in certain styles, or inconsistent use of “et al.”). Proper and nearly error-free bibliography usage contributes to a good style score​

Appendices:
Appendices should be present if they are referenced in the main text. They should be labeled Appendix A, B, C, etc., each with a title. For example: “Appendix A – Ethics Approval Form”, “Appendix B – Sample Data and Features”, “Appendix C – Excerpt of Code Listing”.
Ensure that any crucial information that was deferred from the main text is indeed included here. The report should be complete with the appendices (for instance, if the methodology said “see Appendix B for detailed experimental settings,” Appendix B must exist with those details).
Appendices are optional and should only contain supplementary information. The core report must remain understandable without having to constantly refer to appendices (they are for reference or deeper insight).
The content in appendices should be neatly formatted. If it’s code, it should be in a monospaced font, properly indented; if it’s a questionnaire or similar, formatted for readability. Large tables or raw data can be included, but consider if they’re necessary.
Appendices do not count towards the main word count (generally), but excessive length should be avoided. Each appendix item should be cited at least once in the report body (the automated check can flag any appendix that isn’t referenced in the narrative).
General Quality and Compliance Checks:
Section Order and Titles: The report must include all the major sections in the order listed above: Introduction, Literature Review, Methodology, Results, Discussion, Conclusion, References (appendices as needed). The titles can slightly vary (e.g., “Results and Discussion” combined), but the functionality of each section must be present. An automated check can verify that key terms like “Introduction”, “Literature”, “Methodology/Method/Approach”, “Results”, “Discussion”, “Conclusion”, and “References” appear as section headings. If any are missing, the report structure is incomplete.
Alignment with Objectives: The content of Results/Discussion should align with the objectives stated. The system might look for keywords from the objectives in those sections or check if each objective is addressed by at least some discussion. This ensures the report stays focused and coherent.
Clarity and Logical Flow: While harder to automate fully, some proxies can be used: e.g., checking paragraph length (overly long paragraphs might indicate poor structure), checking for use of transition words (“however, therefore, additionally”) indicating flow of argument. The presence of subheadings in long sections (like Literature Review or Methodology) suggests logical organization.
Figures and Tables usage: Ensure that all figures and tables are numbered and referenced. An automated check can scan for tags like “Figure 1” and see if “Figure 1” is mentioned in the text. Similarly for tables. Also, every figure/table should have a caption (the presence of captions can be checked by looking for text immediately following figure labels).
Critical Evaluation Elements: The Discussion section should contain comparison to literature (the system could check if citations are present in the Discussion section – a discussion with zero citations might indicate lack of comparison with existing work). It should also contain conclusion words (e.g., “therefore, thus, suggests that”) indicating the drawing of conclusions.
Writing Quality: Automated grammar and spell-check tools should be run to catch obvious errors. The system can enforce that the report is written in third person (for instance, flag excessive use of “I” or “we” unless the style guide allows first person). It can also flag colloquial phrases or contractions as potential issues for formal writing.
Ethical Compliance: Check that the report mentions ethical approval or states it’s not needed. The system can search for keywords like “ethics”, “approval”, “consent” to ensure the student has addressed this. Not mentioning ethics at all could be a problem if the project did involve data on individuals.
Honour Code Presence: Verify that the exact required honour code text is included in the preamble (match against the known string from the handbook​

). Also ensure the student’s name and date accompany it, as unsigned declaration might be considered incomplete.
License (if required): If the module expects a Creative Commons license notice, check for “Creative Commons” text in the preamble. Since it’s advised but not compulsory​

, it’s not mandatory; however, including it is good practice.
No Missing Elements: The automated checks collectively ensure that no key component of the final report is missing or out of place. For example, if no abstract is found, or no reference list is detected, those are critical omissions. Each required section carries weight in the assessment (e.g., missing an abstract or a conclusion would significantly impact the “Structure and presentation” score).
